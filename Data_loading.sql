/*

Here comes some thoughts on data loading.

I was frequently asked to load data into database for different purposes. I usually
do this way:
    1. Convert data to be loaded into a csv file;
    2. Create a control file;
    3. Create a temp table;
    4. Invoke sqlldr to load that csv into temp table;
    5. Develop main process script to load data;
    6. Housekeeping.

This works fine with me. However, one day, a script came to my attention that it does
in an alternative way:
    1. Create main process function like:
*/

DECLARE
    TYPE T_RECTYPE IS RECORD(
        ID   NUMBER,
        NAME VARCHAR2(150));
    TYPE T_RECTAB IS TABLE OF T_RECTYPE;

    V_RECS T_RECTAB DEFAULT T_RECTAB(
            data_pattern                                     
                                     );

BEGIN

    FOR IDX IN 1 .. V_RECS.COUNT LOOP
        DBMS_OUTPUT.PUT_LINE(V_RECS(IDX).ID || ' -> ' || V_RECS(IDX).NAME);
    END LOOP;

EXCEPTION
    WHEN OTHERS THEN
        ROLLBACK;
        DBMS_OUTPUT.PUT_LINE(SQLERRM);
        DBMS_OUTPUT.PUT_LINE(DBMS_UTILITY.FORMAT_ERROR_STACK);
        DBMS_OUTPUT.PUT_LINE(DBMS_UTILITY.FORMAT_ERROR_BACKTRACE());
END;

/*
    2. Create a powershell script to read data from a csv, parse data into 
    something like below and then replace 'data_pattern' in script of 1. 
            T_RECTYPE(1, 'one'), T_RECTYPE(2, 'two')
    3. Invoke the script generated by step 2.

It is elegant, but I beg to differ. I could image that if something in data 
contains a single quote, then the script generated might break. There are
bound to be more scenarios that would break this auto-generated script. I 
believe the developer of sqlldr is much, much more smarter than I am. I would
invest time on developing control file to 'wash' data, not on a powershell 
script.

*/